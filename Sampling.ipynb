{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c37aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import polars as pl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8759d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 147.03 GB\n"
     ]
    }
   ],
   "source": [
    "file_path = Path('data/messages.csv')\n",
    "size_in_bytes = file_path.stat().st_size\n",
    "size_in_gb = size_in_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"File size: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d82dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "  id: Int64\n",
      "  message_id: String\n",
      "  campaign_id: Int64\n",
      "  message_type: String\n",
      "  client_id: Int64\n",
      "  channel: String\n",
      "  category: String\n",
      "  platform: String\n",
      "  email_provider: String\n",
      "  stream: String\n",
      "  date: String\n",
      "  sent_at: String\n",
      "  is_opened: String\n",
      "  opened_first_time_at: String\n",
      "  opened_last_time_at: String\n",
      "  is_clicked: String\n",
      "  clicked_first_time_at: String\n",
      "  clicked_last_time_at: String\n",
      "  is_unsubscribed: String\n",
      "  unsubscribed_at: String\n",
      "  is_hard_bounced: String\n",
      "  hard_bounced_at: String\n",
      "  is_soft_bounced: String\n",
      "  soft_bounced_at: String\n",
      "  is_complained: String\n",
      "  complained_at: String\n",
      "  is_blocked: String\n",
      "  blocked_at: String\n",
      "  is_purchased: String\n",
      "  purchased_at: String\n",
      "  created_at: String\n",
      "  updated_at: String\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/38ht234d5p50ytp9mrxf80nc0000gn/T/ipykernel_32054/3224962332.py:5: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  schema = lazy_df.schema\n"
     ]
    }
   ],
   "source": [
    "# This doesn't load any data\n",
    "lazy_df = pl.scan_csv('data/messages.csv')\n",
    "\n",
    "# Get column names and types\n",
    "schema = lazy_df.schema\n",
    "\n",
    "print(\"Column names:\")\n",
    "for col_name, col_type in schema.items():\n",
    "    print(f\"  {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104e609",
   "metadata": {},
   "source": [
    "Since our dataset is very large and involves complex operations such as counting unique values and performing joins, DuckDB is a safer choice for processing on a single machine. DuckDB supports disk spilling, which allows intermediate results to be offloaded to disk when memory limits are exceeded, helping to prevent out-of-memory (OOM) errors.\n",
    "\n",
    "Because we need more than just random sampling — specifically, obtaining unique client IDs, randomly sampling them, and then merging them back to the full dataset via an inner join — other approaches such as Polars, even with lazy evaluation, are prone to OOM errors on our dataset.\n",
    "\n",
    "Our primary goal is to reduce the dataset size while retaining the most relevant information for our research, and DuckDB provides the necessary scalability and reliability to achieve this efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e071a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL interface, also optimized\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT client_id) \n",
    "    FROM 'data/messages.csv'\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "# ran in 50.9 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a721daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16860044"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique clients in messages raw data\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3871e",
   "metadata": {},
   "source": [
    "Since we have data on 16.8 million clients, analyzing such a large population can be computationally challenging and is not strictly necessary for our research objectives. Therefore, we will keep a subset of 0.5 million clients for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45138cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL interface, also optimized\n",
    "result2 = duckdb.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT client_id) \n",
    "    FROM 'data/client_first_purchase_date.csv'\n",
    "\"\"\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ce4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1854736"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique clients with first_purchase_date\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a5d77",
   "metadata": {},
   "source": [
    "# Reducing File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be4cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 255\n",
    "SAMPLE_PERCENTAGE = (500_000 / 16_860_044) * 100\n",
    "INPUT_CSV = 'data/messages.csv'\n",
    "OUTPUT_PARQUET = 'data/messages_subset.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f601997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.965591311624098"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_PERCENTAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b733ff",
   "metadata": {},
   "source": [
    "## DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ff3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reproducible random subset to data/messages_subset.parquet\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "# Step 1: Randomly select exactly 1 million distinct client_ids (reproducible)\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW selected_clients AS\n",
    "SELECT client_id\n",
    "FROM (\n",
    "    SELECT DISTINCT client_id\n",
    "    FROM read_csv_auto('data/messages.csv')\n",
    ")\n",
    "USING SAMPLE {SAMPLE_PERCENTAGE} PERCENT (bernoulli, {RANDOM_SEED});\n",
    "    \n",
    "\"\"\")\n",
    "# REPEATABLE ({RANDOM_SEED});\n",
    "# Step 2: Keep only those clients and save to Parquet\n",
    "con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM read_csv_auto('data/messages.csv')\n",
    "        WHERE client_id IN (SELECT client_id FROM selected_clients)\n",
    "    ) TO 'data/messages_subset.parquet' (FORMAT 'parquet');\n",
    "\"\"\")\n",
    "\n",
    "print(\"Saved reproducible random subset to data/messages_subset.parquet\")\n",
    "\n",
    "\n",
    "# 2m 17.0s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee1f72",
   "metadata": {},
   "source": [
    "### Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff605001",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "SAMPLE_PERCENTAGE = 0.05  # 5%\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "lazy_df = pl.scan_csv(\"data/messages.csv\")\n",
    "\n",
    "sampled_clients = (\n",
    "    lazy_df.select(\"client_id\")\n",
    "    .unique()\n",
    "    .with_columns(\n",
    "        (pl.lit(RANDOM_SEED).hash() % 10000).alias(\"rand_col\")  # optional deterministic hash\n",
    "    )\n",
    "    .filter(pl.col(\"rand_col\") < SAMPLE_PERCENTAGE * 10000)\n",
    "    .select(\"client_id\")\n",
    ")\n",
    "\n",
    "# Filter original CSV using sampled clients (lazy)\n",
    "filtered_df = lazy_df.join(sampled_clients, on=\"client_id\", how=\"inner\")\n",
    "\n",
    "# Write to parquet using sink_parquet for writing by chunks\n",
    "filtered_df.sink_parquet(\"data/messages_subset_p.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5064ba",
   "metadata": {},
   "source": [
    "This can lead to out-of-memory (OOM) errors because Polars does not support disk spilling. When performing operations such as finding unique values or joining DataFrames, Polars must load the entire relevant data into RAM, as it does not create temporary tables like DuckDB.\n",
    "\n",
    "While the lazy evaluation approach in Polars works well for simple or narrow operations — which can be processed in chunks and written to Parquet incrementally, avoiding OOM — more complex or wide operations require loading whole columns into memory at once. This can become problematic when the dataset is much larger than the available RAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
